---
layout: post
title: "Comment flouer un r√©seau de neuronnes ?"
description: ""
category: 
tags: []
---
<div class="message">
  <p align="justify">
  Allo les chefs. Dans cette recette, nous allons voir comment flouer un r√©seau de neuronnes de la plus simple des mani√®res. Nous allons programmer un r√©seau de neuronnes 
  dont l'objectif est de classifier les chiffres manuscrit de la base de donn√©es <a href="http://yann.lecun.com/exdb/mnist/" > MNIST</a>. 
  Ensuite nous allons <span> fabriquer </span> pour chacun des chiffres de 0 √† 9, la <span> meilleure repr√©sentation </span>; c'est √† dire l'image 
  parfaite que le r√©seau attendrait pour s'activer √† 99% pour chacune des classes. Ce qui sera suprenant dans cette petite exp√©rimentation est que, pour une image qui n'a aucun mais vraiment AUCUNNN sens √† nos yeux, et bien le r√©seau lui est capable de dire si c'est un 0,1... ou 9.
  Ce qui est assez critique comme comportement!  
  </p>
</div>

 Les r√©seaux de neuronnes (RN) sont une technique d'apprentissage machine de type connexioniste inspir√©e du fonctionnement du cerveau humain. Le principe de fonctionnement d'une telle architecture est assez simple. De facon br√®ve, voici l'id√©e g√©n√©rale :

> Nous avons un ensemble de donn√©es **X** (commun√©ment appel√© les entr√©es du r√©eau) dont nous voulons pr√©dire les sorties associ√©es **Y** (**Y** peut √™tre √©gale √† **X** dans certaines architectures de RN comme les auto-encoders), le but est de minimiser l'erreur de pr√©diction qui est fonction de Y et de 
**Y'** (sortie pr√©dit pas le r√©seau). L'erreur **E** peut prendre plusieurs forme mais en g√©n√©ral, elle repr√©sente la diff√©rence entre le r√©el **Y** et le pr√©dit **Y'**. On peut donc conclure que plus **E** est petit, le mieux c'est. L'aapprentissage vise √† trouver la configuration du r√©seau (les poids **W**<sub><b>i</b></sub> **i** &#8712; &#925;) qui minimise cette erreur.
L'algorithme le plus utilis√© pour cet apprentissage est la d√©scente du gradient.
. 

Vous trouverez sur mon [d√©p√¥t git](https://github.com/angetato/Optimizers-for-Tensorflow), quelques algorithmes d'optimisation dans les RN utilisant la descente du gradient. 

![img](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Csigma%7D%7B%5Cmu%7D) 

# <a name="part1"></a>Partie 1: Architecture du r√©seau √† flouer

Le code complet de cette recette se trouve [ici](#). J'utilise **Python** et **Tensorflow**, vous devez donc les avoir sur votre machine pour pouvoir ex√©cuter le code.
Consid√©rons le r√©seau de neurones suivant √† 2 couches (Entr√©es + Sorties)
![Source](https://ml4a.github.io/images/figures/mnist_1layer.png)
[Source](https://ml4a.github.io/images/figures/mnist_1layer.png)
Il y'a 784 neurones en entr√©e puisque, chaque image de la BD mnist est encod√©e en une matrice de 28x28 pixels = 784 pixels (si on aplati l'image). En sortie, nous avons 10 neurones, chacun repr√©sentant les chiffres de 0 √† 9. Par exemple, si le chiffre en entr√©e est 0 alors, le neurone charg√© de reconna√Ætre ce chiffre devra s'activer plus que les autres.  
**Remarque :** Pour ceux qui ma√Ætrise le codage d'une architecture neuronale avec tensorflow, vous pouvez vous rendre directement √† la partie [2](#part2).
Voici le code python pour charger la BD : 

```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
```
Une fois la BD charg√©e dans la variable *mnist*, nous allons d√©finir les param√®tres d'entra√Ænement de notre mod√®le. **Remarque:** Je n'ai pas choisis les valeurs optimales, mon but ici n'est pas d'obtenir les meilleurs r√©sultat de classification ...
```python
# Training Parameters
learning_rate = 0.001 # le pas d'apprentissage : une petite valeur implique des pas plus petis vers la solution et vice versa. Je ferais une recette bient√¥t sur l'importance de cette valeur et comment la param√©trer.
num_steps = 30000 
batch_size = 1000 # taille des donn√©es de training que le r√©seau devra voir avant de mettre √† jour les poids.
display_step = 10 # nous allons afficher le r√©sultat courant apr√®s chaque 10 step.

# Network Parameters
num_input = 784 # MNIST data input (img shape: 28*28)
num_out = 10 # 10 classes (0...9)
```
Les variables du r√©seau :
```python
X = tf.placeholder("float", [None, num_input]) #None sera remplac√© par la taille du batchsize.
Y = tf.placeholder(tf.float32, shape=[None, 10]) # Le vecteur de sortie.
weights =  tf.Variable(tf.random_normal([num_input, num_out])) # les poids
```
La sortie est sous la forme d'un vecteur √† 10 entr√©es (10 neurones en sorties). √Ä chaque image correspond un vecteur de sortie. Par exemple pour une image correspondant √† **1**, on aura **[0,1,0,0,0,0,0,0,0,0]** alors que pour une image correspondant √† **9**, on aura **[0,0,0,0,0,0,0,0,0,1]**. 
Le calcul de pr√©diction, de l'erreur et de l'accuracy de notre r√©seau se fait comme sut :
```python
# Prediction
out =  tf.matmul(X, weights) # notre Y'

# Define loss and optimizer, minimize the squared error
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=out))
train_step = tf.train.AdamOptimizer(0.002).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
```
Une fois tout ceci fait, nous pouvons entra√Æner et tester notre mod√®le. 
```python
    # Training
    for i in range(1, num_steps+1):
        # Prepare Data
        # Get the next batch of MNIST data (images and labels)
        batch = mnist.train.next_batch(batch_size)
        # Run optimization op (backprop), cost op (to get loss value) and accuracy opp
        _, l, acc = sess.run([train_step,cross_entropy, accuracy], feed_dict={X: batch[0], Y: batch[1]})
        # Display logs per step
        if i % display_step == 0 or i == 1:
            print(' Step %i: Minibatch Loss: %f accuracy : %f ' % (i, l, acc))
```
Nous n'allons pas tester notre mod√®le sur le _test set_ de la BD. Apr√®s 30000 √©tapes, nous avons obtenus une accuracy d'environ 94% sur le _training set_ . Le mod√®le ainsi concu, est capable √† partir d'une image d'un chiffre, de pr√©dire avec 94% le chiffre √©crit. Que se passe t-il si aucun chiffre n'est √©crit sur l'image ? 

# <a name="part2"></a>Partie 2: Trompons notre r√©seau 

Le mod√®le que nous avons d√©velopp√© dans la partie [1](#part1) est capable de nous dire avec 94% d'accuracy le chiffre qui est contenu dans une image en entr√©e. Si on lui donne une image ne contenant pas de chiffre, il devrait √™tre capable de nous dire qu'il n'y a pas de chiffre √©crit dans l'image. Cependant, la r√©alit√© est tout autre ! M√™me si il n'y a rien dans une image que vous donnez en entr√© √† un r√©seau, il vous donnera toujours un r√©sultat (0...9). La bonne nouvelle est que, ce r√©sultat est sous forme de tableau de probabilit√©. 
Regardons cette image pris d'[ici](https://ml4a.github.io/images/figures/mnist-mistakes.png):
![](https://ml4a.github.io/images/figures/mnist-mistakes.png)
On voit que, le r√©seau attribut des probabilit√©s d'appartenance √† chacun des 10 chiffres pour chaque image. On s'attends donc √† ce que, pour une image ne contenant aucun chiffre, le r√©seau attribut des probabilit√©s tr√®s faibles pour chacun des chiffres sachant qu'il n'a pas appris √† dire non ("qu' aucun chiffre n'est pr√©sent"). On pourrait penser √† rajouter un neurone en sortie pour les cas ou l'image ne contient aucun chiffre (elle peut contenir un chat, une maison, etc.). N√©anmoins, ce dernier cas de figure soul√®verait 3 probl√®mes :
* Trouver des exemples √©tiquet√©s pour pouvoir entra√Æner le r√©seau sur des images ne contenant pas de chiffres.
* Quelles images devront √™tre prises en consid√©ration ? (les images contenant des chats ? des chiens ? des maisons ? ... impossible de d√©terminer les images qui aideront le r√©seau √† distinguer celles qui contiennent des chiffres de celles qui n'en contiennent pas, il est impossible de couvrir tous les cas possibles d'images).
* Quelle quantit√© d'images faudra t-il au r√©seau pour qu'il puisse bien g√©n√©raliser (_ca c'est un probl√®me g√©n√©ral de l'apprentissage machine_ ).

Je propose dans la conclusion, des pistes de r√©flexion pour une possible solution. 

Revenons √† nos moutons üêë ... Nous allons donner √† notre r√©seau ce qu'il veut !
## Qu' apprend le r√©seau ?

Nous allons visualiser ce que chaque neurones de la couche cach√©e (notre couche de sortie) apprends. Soit la [figure](https://ml4a.github.io/ml4a/looking_inside_neural_nets/) suivante :
![](https://ml4a.github.io/images/figures/weights_analogy_1.png)
Chaque neurone de la couche cach√©e

## Est il vraiment intelligent notre RNs?

All your files are listed in the file explorer. You can switch from one to another by clicking a file in the list.

# Conclusion
HTML defines a long list of available inline tags, a complete list of which can be found on the [Mozilla Developer Network](https://developer.mozilla.org/en-US/docs/Web/HTML/Element). (E & Huang 2001)[^Huang2001]

References
--------------------------
[^Huang2001]: E, W. & Huang, Z., 2001. Matching Conditions in Atomistic-Continuum Modeling of Materials. _arXiv.org_, (13), p.135501. Available at: [http://arxiv.org/abs/cond-mat/0106615v1](http://arxiv.org/abs/cond-mat/0106615v1).
